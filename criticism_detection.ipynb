{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# criticism_detection.ipynb\n",
    "\n",
    "### 1 ###\n",
    "•\tIt analysis papers citing another one that has been retracted.\n",
    "•\tIt takes papers that triggered the retraction and classify them as highly_critical.\n",
    "•\tPapers published a year before the first highly_critical paper are classified as non_critical.\n",
    "•\tPapers published after the highly_critical papers are classified as critical (they are not used in the analysis later on because they may contain mixed criticism).\n",
    "•\tDate of publication of the papers are taken from the “PMC-ids.csv.gz” file, available through the PMC FTP service:\n",
    "https://www.ncbi.nlm.nih.gov/pmc/pmctopmid/\n",
    "\n",
    "### 2 ###\n",
    "•\tIt takes all the sentences citing a particular DOI in all the papers disregarding its section. \n",
    "•\tIt shows that clustering algorithms such as PCA and TSNE fail if trying to cluster the sentences from non_critical and high_critical papers just using vader lexicon.\n",
    "\n",
    "### 3 ###\n",
    "•\tIt gets all the sentences in the high_critical papers and a similar number of sentences from the non_critical papers. I finds the words and bigrams used in the high_critical set and are not used in the non_critical set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"./modules\")\n",
    "import words_frec_analysis_get_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "\n",
    "ds_name = 'DOI_cited_science_1179052_retracted'\n",
    "\n",
    "# The paper was retracted based on:\n",
    "DOI_high_critical = [\n",
    "    '10.1186/1742-4690-7-63',\n",
    "    '10.1371/journal.pone.0008519',\n",
    "    '10.1136/bmj.c1018',\n",
    "    '10.1186/1742-4690-7-10',\n",
    "    '10.1371/journal.pone.0008519',\n",
    "    '10.1186/1743-422X-7-224'\n",
    "]\n",
    "\n",
    "analysis_path = os.path.join(data_path, 'analysis')\n",
    "\n",
    "# In\n",
    "citing_sections_tsv = os.path.join(analysis_path, '%s_sections.tsv' % ds_name)\n",
    "pmc_ids_csv = os.path.join(data_path, 'PMC_ids/PMC-ids.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Vader data for sentiment analysis\n",
    "nltk.download('vader_lexicon') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 ###\n",
    "\n",
    "•\tIt analysis papers citing another one that has been retracted.\n",
    "•\tIt takes papers that triggered the retraction and classify them as highly_critical.\n",
    "•\tPapers published a year before the first highly_critical paper are classified as non_critical.\n",
    "•\tPapers published after the highly_critical papers are classified as critical (they are not used in the analysis later on because they may contain mixed criticism).\n",
    "•\tDate of publication of the papers are taken from the “PMC-ids.csv.gz” file, available through the PMC FTP service:\n",
    "https://www.ncbi.nlm.nih.gov/pmc/pmctopmid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    citing_sections_tsv,\n",
    "    sep='\\t',\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "#  df['cited_DOI', 'cited_in_conclusions','cited_in_discussion',\n",
    "#    'cited_in_introduction', 'cited_in_maintext', 'citing_DOI',\n",
    "#    'conclusions_found', 'discussion_found', 'introduction_found',\n",
    "#    'maintext_found', 'reference_id', 'sentence_citing_conclusions',\n",
    "#    'sentence_citing_discussion', 'sentence_citing_intro', 'sentence_citing_maintext']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids = pd.read_csv(pmc_ids_csv, sep=',', encoding='utf-8', low_memory=False)\n",
    "\n",
    "\n",
    "df_ids_select = df_ids[df_ids['DOI'].isin(df.citing_DOI)]\n",
    "\"\"\"\n",
    "for element in df.citing_DOI:\n",
    "    df_ids['DOI'].isin(df.citingDOI)\n",
    "    df_ids_selection2.append(df_ids[[df_ids[\"DOI\"] == element]])\n",
    "    #print(element)\n",
    "\"\"\"\n",
    "\n",
    "df_ids_select.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The paper was retracted based on:\n",
    "#DOI_high_critical = ['10.1186/1742-4690-7-63', '10.1371/journal.pone.0008519', '10.1136/bmj.c1018', '10.1186/1742-4690-7-10', '10.1371/journal.pone.0008519', '10.1186/1743-422X-7-224']\n",
    "\n",
    "df_ids_select_info_highCritical = df_ids[df_ids[\"DOI\"].isin(DOI_high_critical)]\n",
    "\n",
    "df_ids_select_nonCritical = df_ids[((df_ids[\"Year\"] < 2011)) & (df_ids['DOI'].isin(DOI_high_critical) == False)]\n",
    "\n",
    "df_ids_select_Critical = df_ids[(df_ids[\"Year\"] >= 2011) | (df_ids['DOI'].isin(DOI_high_critical))]\n",
    "\n",
    "\n",
    "df_highCritical =df[df['citing_DOI'].isin(df_ids_select_info_highCritical['DOI'])]\n",
    " \n",
    "df_nonCritical = df[df['citing_DOI'].isin(df_ids_select_nonCritical['DOI'])]\n",
    "df_Critical = df[df['citing_DOI'].isin(df_ids_select_Critical['DOI'])]\n",
    "#df_critical\n",
    "\n",
    "df_highCritical_text_part1 = df_highCritical[['citing_DOI','sentence_citing_conclusions']].rename(columns = {'sentence_citing_conclusions': 'text'})\n",
    "df_highCritical_text_part2 = df_highCritical[['citing_DOI','sentence_citing_discussion']].rename(columns = {'sentence_citing_discussion': 'text'})\n",
    "df_highCritical_text_part3 = df_highCritical[['citing_DOI','sentence_citing_intro']].rename(columns = {'sentence_citing_intro': 'text'})\n",
    "df_full_highCritical = pd.concat([df_highCritical_text_part1, df_highCritical_text_part2, df_highCritical_text_part3]).dropna().reset_index(drop=True)\n",
    "df_full_highCritical['label'] = 'high_critical'\n",
    "\n",
    "df_part1 = df_nonCritical[['citing_DOI','sentence_citing_conclusions']].rename(columns = {'sentence_citing_conclusions': 'text'})\n",
    "df_part2 = df_nonCritical[['citing_DOI','sentence_citing_discussion']].rename(columns = {'sentence_citing_discussion': 'text'})\n",
    "df_part3 = df_nonCritical[['citing_DOI','sentence_citing_intro']].rename(columns = {'sentence_citing_intro': 'text'})\n",
    "df_full_nonCritical = pd.concat([df_part1, df_part2, df_part3]).dropna().reset_index(drop=True)\n",
    "df_full_nonCritical['label'] = 'non_critizising'\n",
    "\n",
    "df_part1 = df_Critical[['citing_DOI','sentence_citing_conclusions']].rename(columns = {'sentence_citing_conclusions': 'text'})\n",
    "df_part2 = df_Critical[['citing_DOI','sentence_citing_discussion']].rename(columns = {'sentence_citing_discussion': 'text'})\n",
    "df_part3 = df_Critical[['citing_DOI','sentence_citing_intro']].rename(columns = {'sentence_citing_intro': 'text'})\n",
    "df_full_Critical = pd.concat([df_part1, df_part2, df_part3]).dropna().reset_index(drop=True)\n",
    "df_full_Critical['label'] = 'critizising'\n",
    "\n",
    "df_full = pd.concat([df_full_nonCritical, df_full_Critical]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_highCritical.shape; \", df_highCritical.shape)\n",
    "print(\"df_Critical.shape; \", df_Critical.shape)\n",
    "print(\"df_nonCritical.shape: \", df_nonCritical.shape)\n",
    "print(\"df_full.shape: \", df_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_highCritical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 ###\n",
    "\n",
    "•\tIt takes all the sentences citing a particular DOI in all the papers disregarding its section. \n",
    "•\tIt shows that clustering algorithms such as PCA and TSNE fail if trying to cluster the sentences from non_critical and high_critical papers just using vader lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use vader_lexicon and pca and tsne to cluster papers criticising and not criticising. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction algorithms can be pretty slow, so let's work with a sample\n",
    "# try on the whole data set if you want!\n",
    "\n",
    "# list of colours for making nice plots later\n",
    "COLOURS = ['#E91D0E', '#00A6EF']\n",
    "\n",
    "\n",
    "def scatter(x, label, selected_labels, selected_colors):\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    for selected_label, selected_color in zip(selected_labels, selected_colors):\n",
    "        x_selected = x[(label == selected_label), :]\n",
    "        ax.scatter(\n",
    "            x_selected[:, 0],\n",
    "            x_selected[:, 1],\n",
    "            c=selected_color,\n",
    "            label=selected_label,\n",
    "            alpha=0.5\n",
    "        )\n",
    "    plt.legend()\n",
    "    \n",
    "    return f, ax\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(df_full['text'])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_comp = pca.fit_transform(tfidf_vectors.toarray())\n",
    "scatter(pca_comp, df_full['label'], ['critizising', 'non_critizising'], COLOURS[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have suggested some parameters below, feel free to experiment\n",
    "tsne = TSNE(perplexity = 800, random_state = 42)\n",
    "\n",
    "tsne_comp = tsne.fit_transform(tfidf_vectors.toarray())\n",
    "\n",
    "scatter(tsne_comp, df_full['label'], ['critizising', 'non_critizising'], COLOURS[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected it fails PCA and TSNE using vader lexicon fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 ###\n",
    "•\tIt gets all the sentences in the high_critical papers and a similar number of sentences from the non_critical papers. I finds the words and bigrams used in the high_critical set and are not used in the non_critical set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyse the sentences where the paper is cited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"df_highCritical.shape; \", df_highCritical.shape)\n",
    "#print(\"df_Critical.shape; \", df_Critical.shape)\n",
    "#print(\"df_nonCritical.shape: \", df_nonCritical.shape)\n",
    "#print(\"df_full.shape: \", df_full.shape)\n",
    "\n",
    "#file_df = '/project/elife/data/analysis/df_science1179052_retracted.csv'\n",
    "\n",
    "#file_df = '/project/elife/data/analysis/df_1000_1000v2_prep_.csv'\n",
    "#df = pd.read_csv(file_df, sep='\\t', header = None, encoding='utf-8', names = ['cited_DOI','cited_in_conclusions','cited_in_discussion','cited_in_introduction', 'cited_in_maintext', 'citing_DOI','conclusions_found', 'discussion_found', 'introduction_found','maintext_found', 'reference_id', 'sentence_citing_conclusions','sentence_citing_discussion', 'sentence_citing_intro', 'sentence_citing_maintext'])\n",
    "#  df['cited_DOI', 'cited_in_conclusions','cited_in_discussion',\n",
    "#    'cited_in_introduction', 'cited_in_maintext', 'citing_DOI',\n",
    "#    'conclusions_found', 'discussion_found', 'introduction_found',\n",
    "#    'maintext_found', 'reference_id', 'sentence_citing_conclusions',\n",
    "#    'sentence_citing_discussion', 'sentence_citing_intro', 'sentence_citing_maintext']\n",
    "\n",
    "df_full_highCritical_sample = df_full_highCritical # All the papers the retraction is based on\n",
    "#df_full_highCritical_sample = df_full_Critical.sample(4, random_state = 1)\n",
    "\n",
    "sentences_highCritical = df_full_highCritical_sample.text.dropna()\n",
    "frequent_words_highCritical = words_frec_analysis_get_sentence.analysis_nolimit(sentences_highCritical)\n",
    "\n",
    "# I get a similar number of nonCritical text paragraphs:\n",
    "df_full_nonCritical_sample = df_full_nonCritical.sample(2*sentences_highCritical.shape[0], random_state = 10)\n",
    "\n",
    "sentences_nonCritical = df_full_nonCritical_sample.text.dropna()\n",
    "frequent_words_nonCritical = words_frec_analysis_get_sentence.analysis_nolimit(sentences_nonCritical)\n",
    "\n",
    "\n",
    "set_words_nonCritical = set(frequent_words_nonCritical)\n",
    "set_words_highCritical = set(frequent_words_highCritical)\n",
    "\n",
    "words_only_highCritical = set_words_highCritical - set_words_nonCritical\n",
    "\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(\"Set of words only found in the highCritical paragraphs\")\n",
    "print(words_only_highCritical)\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "words_only_nonCritical = set_words_nonCritical - set_words_highCritical\n",
    "\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(\"Set of words only found in the nonCritical paragraphs\")\n",
    "print(words_only_nonCritical)\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bigrams_highCritical = words_frec_analysis_get_sentence.analyse_bigrams(sentences_highCritical)\n",
    "bigrams_nonCritical =  words_frec_analysis_get_sentence.analyse_bigrams(sentences_nonCritical)    \n",
    "\n",
    "set_bigrams_highCritical = set(bigrams_highCritical)\n",
    "set_bigrams_nonCritical = set(bigrams_nonCritical)\n",
    "\n",
    "bigrams_only_highCritical = set_bigrams_highCritical-set_bigrams_nonCritical\n",
    "\n",
    "\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(\"Set of bi-grams only found in the highCritical paragraphs\")\n",
    "print(bigrams_only_highCritical)\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "\n",
    "### Let's check the frequency of apereance of the words and bigrams highCritical in all the papers###\n",
    "#df_full_Critical['mark'] =\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOME CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_highCritical[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./english_words/wordsEn.txt', 'r') as word_file:\n",
    "    english_words = list(word.strip().lower() for word in word_file)\n",
    "#english_words[1000:1005]\n",
    "\n",
    "if 'workload' in english_words:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentences_highCritical[0:4]\n",
    "sentence.to_csv(sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words_nonCritical[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words_highCritical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_nonCritical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_nonCritical.sample(4, random_state = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
