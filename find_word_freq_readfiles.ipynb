{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency analysis and sentiment with NLTK\n",
    "\n",
    "find_word_freq_readfiles.ipynb\n",
    "\n",
    "### 1 ###\n",
    "•\tIt reads the dataframe with the analysis (file_df_analysis) and the dataframe with the histogram (file_df_analysis_hist) information created by “analyse_papers.ipynb”\n",
    "•\tFrequency analysis of the sentence were a paper was cited. It finds a representative sentence showing how the a paper has been cited in the different sections. It is selected by getting the list of sentences containing the maximum number of most frequent words used when citing the paper (in each section). If there are several sentences it picks the shortest one.\n",
    "\n",
    "### 2 ###\n",
    "•\tSentiment analysis using NLTK.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1 ###\n",
    "•\tIt reads the dataframe with the analysis (file_df_analysis) and the dataframe with the histogram (file_df_analysis_hist) information created by “analyse_papers.ipynb”\n",
    "•\tFrequency analysis of the sentence were a paper was cited. It finds a representative sentence showing how the a paper has been cited in the different sections. It is selected by getting the list of sentences containing the maximum number of most frequent words used when citing the paper (in each section). If there are several sentences it picks the shortest one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deuser/.py-envs/py3/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"./modules\")\n",
    "import words_frec_analysis_get_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "\n",
    "ds_name = 'DOI_cited_science_1179052_retracted'\n",
    "\n",
    "analysis_path = os.path.join(data_path, 'analysis')\n",
    "\n",
    "# In\n",
    "citing_sections_tsv = os.path.join(analysis_path, '%s_sections.tsv' % ds_name)\n",
    "analysis_hist_tsv = os.path.join(analysis_path, '%s_hist_v2.tsv' % ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/deuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/deuser/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One time download\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cited_DOI</th>\n",
       "      <th>cited_in_conclusions</th>\n",
       "      <th>cited_in_discussion</th>\n",
       "      <th>cited_in_introduction</th>\n",
       "      <th>cited_in_maintext</th>\n",
       "      <th>citing_DOI</th>\n",
       "      <th>conclusions_found</th>\n",
       "      <th>discussion_found</th>\n",
       "      <th>introduction_found</th>\n",
       "      <th>maintext_found</th>\n",
       "      <th>reference_number</th>\n",
       "      <th>sentence_citing_conclusions</th>\n",
       "      <th>sentence_citing_discussion</th>\n",
       "      <th>sentence_citing_intro</th>\n",
       "      <th>sentence_citing_maintext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1126/science.1179052</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>DOI not found</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1126/science.1179052</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>10.1371/journal.pone.0027870</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>pone.0027870-Lombardi1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['In 2009, Lombardi&lt;italic&gt;et al.&lt;/italic&gt;repo...</td>\n",
       "      <td>['For amplification of XMRV/MLV&lt;italic&gt;gag&lt;/it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 cited_DOI  cited_in_conclusions  cited_in_discussion  \\\n",
       "0  10.1126/science.1179052                 False                False   \n",
       "1  10.1126/science.1179052                 False                False   \n",
       "\n",
       "   cited_in_introduction  cited_in_maintext                    citing_DOI  \\\n",
       "0                  False              False                 DOI not found   \n",
       "1                   True               True  10.1371/journal.pone.0027870   \n",
       "\n",
       "   conclusions_found  discussion_found  introduction_found  maintext_found  \\\n",
       "0              False             False               False            True   \n",
       "1              False              True                True            True   \n",
       "\n",
       "         reference_number  sentence_citing_conclusions  \\\n",
       "0                      B2                          NaN   \n",
       "1  pone.0027870-Lombardi1                          NaN   \n",
       "\n",
       "   sentence_citing_discussion  \\\n",
       "0                         NaN   \n",
       "1                         NaN   \n",
       "\n",
       "                               sentence_citing_intro  \\\n",
       "0                                                NaN   \n",
       "1  ['In 2009, Lombardi<italic>et al.</italic>repo...   \n",
       "\n",
       "                            sentence_citing_maintext  \n",
       "0                                                NaN  \n",
       "1  ['For amplification of XMRV/MLV<italic>gag</it...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(citing_sections_tsv, sep='\\t', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI_cited</th>\n",
       "      <th>cited_in_conlusions</th>\n",
       "      <th>cited_in_discussion</th>\n",
       "      <th>cited_in_introduction</th>\n",
       "      <th>cited_in_maintext</th>\n",
       "      <th>papers_all_sections_found</th>\n",
       "      <th>total_papers_citing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1126/science.1179052</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 DOI_cited  cited_in_conlusions  cited_in_discussion  \\\n",
       "0  10.1126/science.1179052                    0                    0   \n",
       "\n",
       "   cited_in_introduction  cited_in_maintext  papers_all_sections_found  \\\n",
       "0                    1.0                1.0                          1   \n",
       "\n",
       "   total_papers_citing  \n",
       "0                    2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hist = pd.read_csv(analysis_hist_tsv, sep='\\t', encoding='utf-8')\n",
    "df_hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cited_DOI</th>\n",
       "      <th>cited_in_conclusions</th>\n",
       "      <th>cited_in_discussion</th>\n",
       "      <th>cited_in_introduction</th>\n",
       "      <th>cited_in_maintext</th>\n",
       "      <th>citing_DOI</th>\n",
       "      <th>conclusions_found</th>\n",
       "      <th>discussion_found</th>\n",
       "      <th>introduction_found</th>\n",
       "      <th>maintext_found</th>\n",
       "      <th>reference_number</th>\n",
       "      <th>sentence_citing_conclusions</th>\n",
       "      <th>sentence_citing_discussion</th>\n",
       "      <th>sentence_citing_intro</th>\n",
       "      <th>sentence_citing_maintext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1126/science.1179052</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>DOI not found</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1126/science.1179052</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>10.1371/journal.pone.0027870</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>pone.0027870-Lombardi1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['In 2009, Lombardi&lt;italic&gt;et al.&lt;/italic&gt;repo...</td>\n",
       "      <td>['For amplification of XMRV/MLV&lt;italic&gt;gag&lt;/it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 cited_DOI  cited_in_conclusions  cited_in_discussion  \\\n",
       "0  10.1126/science.1179052                 False                False   \n",
       "1  10.1126/science.1179052                 False                False   \n",
       "\n",
       "   cited_in_introduction  cited_in_maintext                    citing_DOI  \\\n",
       "0                  False              False                 DOI not found   \n",
       "1                   True               True  10.1371/journal.pone.0027870   \n",
       "\n",
       "   conclusions_found  discussion_found  introduction_found  maintext_found  \\\n",
       "0              False             False               False            True   \n",
       "1              False              True                True            True   \n",
       "\n",
       "         reference_number  sentence_citing_conclusions  \\\n",
       "0                      B2                          NaN   \n",
       "1  pone.0027870-Lombardi1                          NaN   \n",
       "\n",
       "   sentence_citing_discussion  \\\n",
       "0                         NaN   \n",
       "1                         NaN   \n",
       "\n",
       "                               sentence_citing_intro  \\\n",
       "0                                                NaN   \n",
       "1  ['In 2009, Lombardi<italic>et al.</italic>repo...   \n",
       "\n",
       "                            sentence_citing_maintext  \n",
       "0                                                NaN  \n",
       "1  ['For amplification of XMRV/MLV<italic>gag</it...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['cited_DOI'] == df_hist.iloc[0].DOI_cited]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    ['For amplification of XMRV/MLV<italic>gag</it...\n",
       "Name: sentence_citing_maintext, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df['sentence_citing_maintext'].dropna()\n",
    "\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cited_DOI</th>\n",
       "      <th>cited_in_conclusions</th>\n",
       "      <th>cited_in_discussion</th>\n",
       "      <th>cited_in_introduction</th>\n",
       "      <th>cited_in_maintext</th>\n",
       "      <th>citing_DOI</th>\n",
       "      <th>conclusions_found</th>\n",
       "      <th>discussion_found</th>\n",
       "      <th>introduction_found</th>\n",
       "      <th>maintext_found</th>\n",
       "      <th>reference_number</th>\n",
       "      <th>sentence_citing_conclusions</th>\n",
       "      <th>sentence_citing_discussion</th>\n",
       "      <th>sentence_citing_intro</th>\n",
       "      <th>sentence_citing_maintext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1126/science.1179052</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>10.1371/journal.pone.0027870</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>pone.0027870-Lombardi1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['In 2009, Lombardi&lt;italic&gt;et al.&lt;/italic&gt;repo...</td>\n",
       "      <td>['For amplification of XMRV/MLV&lt;italic&gt;gag&lt;/it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 cited_DOI  cited_in_conclusions  cited_in_discussion  \\\n",
       "1  10.1126/science.1179052                 False                False   \n",
       "\n",
       "   cited_in_introduction  cited_in_maintext                    citing_DOI  \\\n",
       "1                   True               True  10.1371/journal.pone.0027870   \n",
       "\n",
       "   conclusions_found  discussion_found  introduction_found  maintext_found  \\\n",
       "1              False              True                True            True   \n",
       "\n",
       "         reference_number  sentence_citing_conclusions  \\\n",
       "1  pone.0027870-Lombardi1                          NaN   \n",
       "\n",
       "   sentence_citing_discussion  \\\n",
       "1                         NaN   \n",
       "\n",
       "                               sentence_citing_intro  \\\n",
       "1  ['In 2009, Lombardi<italic>et al.</italic>repo...   \n",
       "\n",
       "                            sentence_citing_maintext  \n",
       "1  ['For amplification of XMRV/MLV<italic>gag</it...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "was_intro_found = df['introduction_found'] == True\n",
    "df_filtered = df[was_intro_found]\n",
    "print(df_filtered.shape)\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_citing_conclusions</th>\n",
       "      <th>sentence_citing_discussion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_citing_conclusions  sentence_citing_discussion\n",
       "count                          0.0                         0.0\n",
       "mean                           NaN                         NaN\n",
       "std                            NaN                         NaN\n",
       "min                            NaN                         NaN\n",
       "25%                            NaN                         NaN\n",
       "50%                            NaN                         NaN\n",
       "75%                            NaN                         NaN\n",
       "max                            NaN                         NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.drop('sentence_citing_intro', axis=1, inplace=False).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's find the most frequente words in an example of a representative sentence for each section (intro, discussion, etc) where the paper is cited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cited_DOI:  ['10.1126/science.1179052']\n",
      "\n",
      "------------- ANALYSIS INTRODUCTION -----------\n",
      "1    reported dection XMRV peripherblood mononuclea...\n",
      "Name: sentence_citing_intro, dtype: object\n",
      "['cells', 'cfs', 'cohort', 'compared', 'controls']\n",
      "Frequent words included:  [5]\n",
      "Maximum words in the same sentence  =  5\n",
      "1,\"['In 2009, Lombardi<italic>et al.</italic>reported the detection of XMRV in both peripheral blood mononuclear cells (PBMC) and plasma of 67% of a CFS patient cohort compared to 3.7% in healthy controls<xref ref-type=\"\"bibr\"\" rid=\"\"pone.0027870-Lombardi1\"\">[5]</xref>.']\"\n",
      "\n",
      "------------- ANALYSIS MAINTEXT -----------\n",
      "1    gagsequences, µl transcribed cDNA used first r...\n",
      "Name: sentence_citing_maintext, dtype: object\n",
      "['amplification', 'round', 'transcribed', 'usb', 'used']\n",
      "Frequent words included:  [5]\n",
      "Maximum words in the same sentence  =  5\n",
      "1,\"['For amplification of XMRV/MLV<italic>gag</italic>sequences, 5 µl of the transcribed cDNA were used for the first round of amplification with primers 419F (<named-content content-type=\"\"gene\"\">5′-ATCAGTTAACCTACCCGAGTCGGAC-3′</named-content>) and 1154R (<named-content content-type=\"\"gene\"\">5′-GCCGCCTCTTCTTCATTGTTCTC-3′</named-content>)<xref ref-type=\"\"bibr\"\" rid=\"\"pone.0027870-Lombardi1\"\">[5]</xref>and HotStart-IT FideliTaq Master Mix (USB) with the recommended component volumes.']\"\n",
      "\n",
      "------------- ANALYSIS DISCUSSION -----------\n",
      "No sentences to analyse\n",
      "\n",
      " ------------- ANALYSIS CONCLUSIONS -----------\n",
      "No sentences to analyse\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(citing_sections_tsv, sep='\\t', encoding='utf-8')\n",
    "#  df['cited_DOI', 'cited_in_conclusions','cited_in_discussion',\n",
    "#    'cited_in_introduction', 'cited_in_maintext', 'citing_DOI',\n",
    "#    'conclusions_found', 'discussion_found', 'introduction_found',\n",
    "#    'maintext_found', 'reference_id', 'sentence_citing_conclusions',\n",
    "#    'sentence_citing_discussion', 'sentence_citing_intro', 'sentence_citing_maintext']\n",
    "\n",
    "\n",
    "print('cited_DOI: ', df.cited_DOI.unique())\n",
    "\n",
    "\n",
    "print(\"\\n------------- ANALYSIS INTRODUCTION -----------\")\n",
    "sentences_intro = df.sentence_citing_intro.dropna()\n",
    "if not sentences_intro.empty:\n",
    "    words_frec_analysis_get_sentence.analysis(sentences_intro)\n",
    "else:\n",
    "    print(\"No sentences to analyse\")\n",
    "\n",
    "print(\"\\n------------- ANALYSIS MAINTEXT -----------\")\n",
    "sentences_maintext = df.sentence_citing_maintext.dropna()\n",
    "if not sentences_maintext.empty:\n",
    "    words_frec_analysis_get_sentence.analysis(sentences_maintext)\n",
    "else:\n",
    "    print(\"No sentences to analyse\")\n",
    "\n",
    "print(\"\\n------------- ANALYSIS DISCUSSION -----------\")\n",
    "sentences_discussion = df.sentence_citing_discussion.dropna()\n",
    "if not sentences_discussion.empty:\n",
    "    words_frec_analysis_get_sentence.analysis(sentences_discussion)\n",
    "else:\n",
    "    print(\"No sentences to analyse\")\n",
    "    \n",
    "    \n",
    "print(\"\\n ------------- ANALYSIS CONCLUSIONS -----------\")\n",
    "#number_features = 5\n",
    "sentences_conclusions = df.sentence_citing_conclusions.dropna()\n",
    "if not sentences_conclusions.empty:\n",
    "    words_frec_analysis_get_sentence.analysis(sentences_conclusions)\n",
    "else:\n",
    "    print(\"No sentences to analyse\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 ###\n",
    "•\tSentiment analysis using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    et al.reported detection XMRV peripheral blood...\n",
      "Name: sentence_citing_intro, dtype: object\n",
      "Frequent words included (intro):  [5]\n",
      "Frequent words included (maintext):  [5]\n",
      "Frequent words included (discussion):  [0.0]\n",
      "Frequent words included (conclusions):  [0.0]\n",
      "maximum words in the same sentence in introduction = 5\n",
      "maximum words in the same sentence in maintext = 5\n",
      "maximum words in the same sentence in discussion = 0.0\n",
      "maximum words in the same sentence in conclusions = 0.0\n",
      "min_length:  265\n",
      "1,\"['In 2009, Lombardi<italic>et al.</italic>reported the detection of XMRV in both peripheral blood mononuclear cells (PBMC) and plasma of 67% of a CFS patient cohort compared to 3.7% in healthy controls<xref ref-type=\"\"bibr\"\" rid=\"\"pone.0027870-Lombardi1\"\">[5]</xref>.']\"\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords\n",
    "\n",
    "def filter_sentence(citing_sentence):\n",
    "    if citing_sentence == None:\n",
    "        return \" \" #filtered_sentences_noNone.append(\" \")\n",
    "    \n",
    "    if citing_sentence != None:\n",
    "        citing_sentence = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", citing_sentence) #to remove citations\n",
    "        #citing_sentence = re.sub(\"[*?]\", \"\", citing_sentence) #to remove citations\n",
    "        citing_sentence = re.sub('[0-9]+', '', citing_sentence)\n",
    "        delete = [\"Introduction\", \"Background\", \"Conclusions\",\"the\", \"and\", \"therefore\", \"thus\"]#, \"\\n\", \"<\\sub>\", \"bibr\", \"ref\", \"rid\", \"type\", \"xref\"] #, \"/p\\np\\n\", \"\\p\"]\n",
    "        for word in delete:\n",
    "            citing_sentence = re.sub(word, \"\", citing_sentence) \n",
    "        #citing_sentence = re.sub(\"\\?\", \"\", citing_sentence) #to remove citations\n",
    "        citing_sentence = ' '.join([word for word in citing_sentence.split() if word not in (stopwords.words('english'))])\n",
    "        return citing_sentence # O ?????????filtered_sentences_noNone.append(sentence[0])\n",
    "\n",
    "def fit_transform_or_empty(vectorizer, data):\n",
    "    return count_vectorizer.fit_transform(data) if len(data) > 0 else csr_matrix([])\n",
    "\n",
    "#file_df = '/project/elife/data/analysis/df_1000_1000v2_prep_.csv'\n",
    "df = pd.read_csv(citing_sections_tsv, sep='\\t', encoding='utf-8')\n",
    "\n",
    "#  df['cited_DOI', 'cited_in_conclusions','cited_in_discussion',\n",
    "#    'cited_in_introduction', 'cited_in_maintext', 'citing_DOI',\n",
    "#    'conclusions_found', 'discussion_found', 'introduction_found',\n",
    "#    'maintext_found', 'reference_id', 'sentence_citing_conclusions',\n",
    "#    'sentence_citing_discussion', 'sentence_citing_intro', 'sentence_citing_maintext']\n",
    "    \n",
    "\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=5)\n",
    "#sentence = ''.join(str(string) for string in citing_sentences_list)\n",
    "#sentence = sentence.decode('utf-8')\n",
    "#u_sentence = unicode( sentence, \"utf-8\" )\n",
    "#backToBytes = u_sentence.encode( \"utf-8\" )\n",
    "\n",
    "\n",
    "\n",
    "#sentence = re.sub(r',([0-9])', '\\\\1', sentence)\n",
    "# sort out HMTL formatting of &\n",
    "#sentence = re.sub(r'&amp', 'and', sentence)\n",
    "\n",
    "citing_sentences_intro_list = df.sentence_citing_intro.dropna()\n",
    "citing_sentences_maintext_list = df.sentence_citing_maintext.dropna()\n",
    "citing_sentences_discussion_list = df.sentence_citing_discussion.dropna()\n",
    "citing_sentences_conclusions_list = df.sentence_citing_conclusions.dropna()\n",
    "    \n",
    "citing_sentences_original_intro = deepcopy(citing_sentences_intro_list)\n",
    "citing_sentences_original_maintext = deepcopy(citing_sentences_maintext_list)\n",
    "citing_sentences_original_discussion = deepcopy(citing_sentences_discussion_list)\n",
    "citing_sentences_original_conclusions = deepcopy(citing_sentences_conclusions_list) # This is a list of lists, so you need deepcopy\n",
    "\n",
    "filtered_sentences_intro_list = citing_sentences_intro_list.apply(filter_sentence)\n",
    "filtered_sentences_maintext_list = citing_sentences_maintext_list.apply(filter_sentence)\n",
    "filtered_sentences_discussion_list = citing_sentences_discussion_list.apply(filter_sentence)\n",
    "filtered_sentences_conclusions_list = citing_sentences_conclusions_list.apply(filter_sentence)        \n",
    "\n",
    "#filtered_sentences_intro_list = [filter_sentence(sentence) for sentence in citing_sentences_intro_list]\n",
    "#filtered_sentences_maintext_list = [filter_sentence(sentence) for sentence in citing_sentences_maintext_list]\n",
    "#filtered_sentences_discussion_list = [filter_sentence(sentence) for sentence in citing_sentences_discussion_list]\n",
    "#filtered_sentences_conclusions_list = [filter_sentence(sentence) for sentence in citing_sentences_conclusions_list]\n",
    "\n",
    "print(filtered_sentences_intro_list)\n",
    "#citing_sentences_intro_list.apply(filter_sentence)\n",
    "\n",
    "count_vectors_intro = fit_transform_or_empty(count_vectorizer, filtered_sentences_intro_list)\n",
    "#print(count_vectorizer.get_feature_names())\n",
    "#word_frequency_intro = count_vectors_intro.toarray()\n",
    "\n",
    "count_vectors_maintext = fit_transform_or_empty(count_vectorizer, filtered_sentences_maintext_list)\n",
    "#print(count_vectorizer.get_feature_names())\n",
    "#word_frequency_maintext = count_vectors_maintext.toarray()\n",
    "\n",
    "count_vectors_discussion = fit_transform_or_empty(count_vectorizer, filtered_sentences_discussion_list)\n",
    "#print(count_vectorizer.get_feature_names())\n",
    "#word_frequency_discussion = count_vectors_discussion.toarray()\n",
    "\n",
    "count_vectors_conclusions = fit_transform_or_empty(count_vectorizer, filtered_sentences_conclusions_list)\n",
    "#print(count_vectorizer.get_feature_names())\n",
    "#word_frequency_conclusions = count_vectors_conclusions.toarray()\n",
    "\n",
    "\n",
    "#print(word_frequency_intro)\n",
    "#print(word_frequency_maintext)\n",
    "#print(word_frequency_discussion)\n",
    "#print(word_frequency_conclusions)\n",
    "\n",
    "number_words_in_sentence_intro = np.sum(count_vectors_intro.toarray(),axis=1).tolist()\n",
    "number_words_in_sentence_maintext = np.sum(count_vectors_maintext.toarray(),axis=1).tolist()\n",
    "number_words_in_sentence_discussion = np.sum(count_vectors_discussion.toarray(),axis=1).tolist()\n",
    "number_words_in_sentence_conclusions = np.sum(count_vectors_conclusions.toarray(),axis=1).tolist()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Frequent words included (intro): \", number_words_in_sentence_intro)\n",
    "print(\"Frequent words included (maintext): \", number_words_in_sentence_maintext)\n",
    "print(\"Frequent words included (discussion): \", number_words_in_sentence_discussion)\n",
    "print(\"Frequent words included (conclusions): \", number_words_in_sentence_conclusions)\n",
    "\n",
    "print(\"maximum words in the same sentence in introduction =\", max(number_words_in_sentence_intro))\n",
    "print(\"maximum words in the same sentence in maintext =\", max(number_words_in_sentence_maintext))\n",
    "print(\"maximum words in the same sentence in discussion =\", max(number_words_in_sentence_discussion))\n",
    "print(\"maximum words in the same sentence in conclusions =\", max(number_words_in_sentence_conclusions))\n",
    "\n",
    "\"\"\"\n",
    "df_sentences_intro = pd.DataFrame({\"citing_intro_sentence\" : filtered_sentences_intro_list, \n",
    "                                 \"number_frequent_words\" : number_words_in_sentence_intro})\n",
    "\"\"\"\n",
    "# length of sentence\n",
    "\n",
    "sentence_length_intro = []\n",
    "for sentence in citing_sentences_intro_list:\n",
    "    if sentence == None:\n",
    "        sentence_length_intro.append(0)\n",
    "    if sentence != None:\n",
    "        sentence_length_intro.append(len(sentence))\n",
    "\n",
    "df_sentences_intro = pd.DataFrame({\"frequent_words\" : number_words_in_sentence_intro,\n",
    "                                \"citing_sentence\" : citing_sentences_original_intro,\n",
    "                                \"sentence_length\": sentence_length_intro})\n",
    "\n",
    "\n",
    "sentences_toCheck = df_sentences_intro[(df_sentences_intro[\"frequent_words\"] == max(df_sentences_intro[\"frequent_words\"]))] #['sentence_citing_intro']\n",
    "min_length= min(sentences_toCheck[\"sentence_length\"][:])\n",
    "print(\"min_length: \", min_length)\n",
    "\n",
    "sentence = sentences_toCheck[sentences_toCheck['sentence_length'] == min_length][\"citing_sentence\"]\n",
    "\n",
    "# To print the full content\n",
    "sentence.to_csv(sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For amplification of XMRV/MLV<italic>gag</italic>sequences, 5 µl of the transcribed cDNA were used for the first round of amplification with primers 419F (<named-content content-type=\"gene\">5′-ATCAGTTAACCTACCCGAGTCGGAC-3′</named-content>) and 1154R (<named-content content-type=\"gene\">5′-GCCGCCTCTTCTTCATTGTTCTC-3′</named-content>)<xref ref-type=\"bibr\" rid=\"pone.0027870-Lombardi1\">[5]</xref>and HotStart-IT FideliTaq Master Mix (USB) with the recommended component volumes.']\n",
      "[nan]\n",
      "['1154r', 'round', 'transcribed', 'usb', 'used']\n",
      "[[0 0 0 0 0]\n",
      " [1 1 1 1 1]]\n",
      "Frequent words included:  [0, 5]\n",
      "maximum words in the same sentence = 5\n",
      "1,\"['For amplification of XMRV/MLV<italic>gag</italic>sequences, 5 µl of the transcribed cDNA were used for the first round of amplification with primers 419F (<named-content content-type=\"\"gene\"\">5′-ATCAGTTAACCTACCCGAGTCGGAC-3′</named-content>) and 1154R (<named-content content-type=\"\"gene\"\">5′-GCCGCCTCTTCTTCATTGTTCTC-3′</named-content>)<xref ref-type=\"\"bibr\"\" rid=\"\"pone.0027870-Lombardi1\"\">[5]</xref>and HotStart-IT FideliTaq Master Mix (USB) with the recommended component volumes.']\"\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(citing_sections_tsv, sep='\\t', encoding='utf-8')\n",
    "\n",
    "citing_sentences_intro= df['sentence_citing_maintext']\n",
    "citing_sentences_intro_list = citing_sentences_intro.tolist()\n",
    "print(citing_sentences_intro_list[1])\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=5)\n",
    "#sentence = ''.join(str(string) for string in citing_sentences_list)\n",
    "#sentence = sentence.decode('utf-8')\n",
    "#u_sentence = unicode( sentence, \"utf-8\" )\n",
    "#backToBytes = u_sentence.encode( \"utf-8\" )\n",
    "\n",
    "\n",
    "\n",
    "#sentence = re.sub(r',([0-9])', '\\\\1', sentence)\n",
    "# sort out HMTL formatting of &\n",
    "#sentence = re.sub(r'&amp', 'and', sentence)\n",
    "\n",
    "# If you want to avoid to go through all the papers, you can select those with introductions by\n",
    "# by replacing citing_sentences_intro_list by df_intro_filtered.tolist()\n",
    "\n",
    "\n",
    "citing_sentences_original = deepcopy(citing_sentences_intro_list) # This is a list of lists, so you need deepcopy\n",
    "#print(citing_sentences_original[19])\n",
    "filtered_sentences_noNone = []\n",
    "filtered_sentences = citing_sentences_intro_list[:]\n",
    "print(filtered_sentences[0:1])\n",
    "for sentence in filtered_sentences:\n",
    "    if (type(sentence) == float):\n",
    "        filtered_sentences_noNone.append(\" \")\n",
    "    if (type(sentence) != float):\n",
    "        #print(\"######################### \", type(sentence), sentence)\n",
    "        delete = [\"Introduction\", \"Background\", \"the\", \"and\"]\n",
    "        for word in delete:\n",
    "            sentence = re.sub(word, \"\", sentence) \n",
    "        sentence = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", sentence) #to remove cititations\n",
    "        sentence = ' '.join([word for word in sentence.split() if word not in (stopwords.words('english'))])\n",
    "        filtered_sentences_noNone.append(sentence)\n",
    "\n",
    "count_vectorizer.fit_transform(filtered_sentences_noNone) #backToBytes\n",
    "#print(citing_sentences_original[19]) \n",
    "    \n",
    "print(count_vectorizer.get_feature_names())\n",
    "count_vectors = count_vectorizer.transform(filtered_sentences_noNone)\n",
    "\n",
    "word_frequency = count_vectors.toarray()\n",
    "print(word_frequency)\n",
    "number_words_in_sentence = np.sum(count_vectors.toarray(),axis=1).tolist()\n",
    "\n",
    "print(\"Frequent words included: \", number_words_in_sentence)\n",
    "\n",
    "\n",
    "#print(citing_sentences_intro_list)\n",
    "print(\"maximum words in the same sentence =\", max(number_words_in_sentence))\n",
    "\n",
    "#df_introCiting = pd.DataFrame({\"citing_intro_sentence\" : filtered_sentences_noNone, \n",
    "#                                \"number_frequent_words\" : number_words_in_sentence})\n",
    "#print(df_introCiting)\n",
    "\n",
    "# length of sentence\n",
    "sentence_length = []\n",
    "for sentence in citing_sentences_intro_list:\n",
    "    if (sentence == None) | (sentence == np.nan):\n",
    "        sentence_length.append(0)\n",
    "    if (sentence != None) & (sentence != np.nan):\n",
    "        sentence_length.append(len(str(sentence)))\n",
    "\n",
    "\n",
    "\n",
    "df_introCiting = pd.DataFrame({\"frequent_words\" : number_words_in_sentence,\n",
    "                                \"citing_sentence\" : citing_sentences_original,\n",
    "                                \"sentence_length\": sentence_length})\n",
    "\n",
    "\n",
    "sentences_toCheck = df_introCiting[(df_introCiting[\"frequent_words\"] == max(df_introCiting[\"frequent_words\"]))] #['sentence_citing_intro']\n",
    "min_length= min(sentences_toCheck[\"sentence_length\"][:])\n",
    "\n",
    "sentence = sentences_toCheck[sentences_toCheck['sentence_length'] == min_length][\"citing_sentence\"]\n",
    "\n",
    "# To print the full content\n",
    "sentence.to_csv(sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     citing_sentence  frequent_words  \\\n",
      "0                                                NaN               0   \n",
      "1  ['For amplification of XMRV/MLV<italic>gag</it...               5   \n",
      "\n",
      "   sentence_length  compound  neg    neu    pos  \n",
      "0                3    0.0000  0.0  0.000  0.000  \n",
      "1              478    0.2023  0.0  0.917  0.083  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor i, tweet in df_sentiment['text'].iteritems():\\n    ss = sid.polarity_scores(str(tweet))\\n    for k in sorted(ss):\\n        df_sentiment.loc[i, k] = ss[k]\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "df_sentiment = df_introCiting.copy()\n",
    "#ss = sid.polarity_scores(df_sentiment['citing_sentence'][4][0]) #if None => error\n",
    "#print(ss)\n",
    "\n",
    "df_sentiment2 = filtered_sentences_noNone\n",
    "#ss2 = sid.polarity_scores(df_sentiment2[2])\n",
    "ss2 = sid.polarity_scores(df_sentiment2[0])\n",
    "\n",
    "sentiment_scores_list = []\n",
    "for sentence in df_sentiment2:\n",
    "    ss2 = sid.polarity_scores(sentence) # from twython  package\n",
    "    sentiment_scores_list.append(ss2)\n",
    "df_sentiment_scores = pd.DataFrame(sentiment_scores_list)\n",
    "\n",
    "    #df_sentiment_scores.append(ss2)\n",
    "#df_sentiment2\n",
    "\n",
    "\n",
    "df_sentiment = df_sentiment.join(df_sentiment_scores)\n",
    "\n",
    "print(df_sentiment)\n",
    "\"\"\"\n",
    "for i, tweet in df_sentiment['text'].iteritems():\n",
    "    ss = sid.polarity_scores(str(tweet))\n",
    "    for k in sorted(ss):\n",
    "        df_sentiment.loc[i, k] = ss[k]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(len(df_sentiment))\n",
    "df_sentiment['compound']\n",
    "\n",
    "#plt.close()\n",
    "#comp = plt.scatter(x,df_sentiment['compound'])\n",
    "pos = plt.scatter(x,df_sentiment['pos'], color = 'g')\n",
    "neu = plt.scatter(x,df_sentiment['neu'], color = 'y')\n",
    "neg = plt.scatter(x,df_sentiment['neg'], color = 'r')\n",
    "\n",
    "plt.legend((pos, neu, neg),('positive', 'neutral','negative'), loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"List of paper\", fontsize=14)\n",
    "plt.ylabel(\"Sentiment score\", fontsize=14)\n",
    "\n",
    "plt.savefig(\"sentiment.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
