{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency analysis and sentiment with NLTK\n",
    "\n",
    "find_word_freq_readfiles.ipynb\n",
    "\n",
    "### 1 ###\n",
    "•\tIt reads the dataframe with the analysis (file_df_analysis) and the dataframe with the histogram (file_df_analysis_hist) information created by “analyse_papers.ipynb”\n",
    "•\tFrequency analysis of the sentence were a paper was cited. It finds a representative sentence showing how the a paper has been cited in the different sections. It is selected by getting the list of sentences containing the maximum number of most frequent words used when citing the paper (in each section). If there are several sentences it picks the shortest one.\n",
    "\n",
    "### 2 ###\n",
    "•\tSentiment analysis using NLTK.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1 ###\n",
    "•\tIt reads the dataframe with the analysis (file_df_analysis) and the dataframe with the histogram (file_df_analysis_hist) information created by “analyse_papers.ipynb”\n",
    "•\tFrequency analysis of the sentence were a paper was cited. It finds a representative sentence showing how the a paper has been cited in the different sections. It is selected by getting the list of sentences containing the maximum number of most frequent words used when citing the paper (in each section). If there are several sentences it picks the shortest one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"./modules\")\n",
    "import words_frec_analysis_get_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "\n",
    "ds_name = 'DOI_cited_science_1179052_retracted'\n",
    "\n",
    "analysis_path = os.path.join(data_path, 'analysis')\n",
    "\n",
    "# In\n",
    "citing_sections_tsv = os.path.join(analysis_path, '%s_sections.tsv' % ds_name)\n",
    "analysis_hist_tsv = os.path.join(analysis_path, '%s_hist_v2.tsv' % ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One time download\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(citing_sections_tsv, sep='\\t', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = pd.read_csv(analysis_hist_tsv, sep='\\t', encoding='utf-8')\n",
    "df_hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cited_DOI'] == df_hist.iloc[0].DOI_cited]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df['sentence_citing_maintext'].dropna()\n",
    "\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "was_intro_found = df['introduction_found'] == True\n",
    "df_filtered = df[was_intro_found]\n",
    "print(df_filtered.shape)\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.drop('sentence_citing_intro', axis=1, inplace=False).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's find the most frequente words in an example of a representative sentence for each section (intro, discussion, etc) where the paper is cited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(citing_sections_tsv, sep='\\t', encoding='utf-8')\n",
    "#  df['cited_DOI', 'cited_in_conclusions','cited_in_discussion',\n",
    "#    'cited_in_introduction', 'cited_in_maintext', 'citing_DOI',\n",
    "#    'conclusions_found', 'discussion_found', 'introduction_found',\n",
    "#    'maintext_found', 'reference_id', 'sentence_citing_conclusions',\n",
    "#    'sentence_citing_discussion', 'sentence_citing_intro', 'sentence_citing_maintext']\n",
    "\n",
    "\n",
    "print('cited_DOI: ', df.cited_DOI.unique())\n",
    "\n",
    "\n",
    "print(\"\\n------------- ANALYSIS INTRODUCTION -----------\")\n",
    "sentences_intro = df.sentence_citing_intro.dropna()\n",
    "if not sentences_intro.empty:\n",
    "    words_frec_analysis_get_sentence.analysis(sentences_intro)\n",
    "else:\n",
    "    print(\"No sentences to analyse\")\n",
    "\n",
    "print(\"\\n------------- ANALYSIS MAINTEXT -----------\")\n",
    "sentences_maintext = df.sentence_citing_maintext.dropna()\n",
    "if not sentences_maintext.empty:\n",
    "    words_frec_analysis_get_sentence.analysis(sentences_maintext)\n",
    "else:\n",
    "    print(\"No sentences to analyse\")\n",
    "\n",
    "print(\"\\n------------- ANALYSIS DISCUSSION -----------\")\n",
    "sentences_discussion = df.sentence_citing_discussion.dropna()\n",
    "if not sentences_discussion.empty:\n",
    "    words_frec_analysis_get_sentence.analysis(sentences_discussion)\n",
    "else:\n",
    "    print(\"No sentences to analyse\")\n",
    "    \n",
    "    \n",
    "print(\"\\n ------------- ANALYSIS CONCLUSIONS -----------\")\n",
    "#number_features = 5\n",
    "sentences_conclusions = df.sentence_citing_conclusions.dropna()\n",
    "if not sentences_conclusions.empty:\n",
    "    words_frec_analysis_get_sentence.analysis(sentences_conclusions)\n",
    "else:\n",
    "    print(\"No sentences to analyse\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 ###\n",
    "•\tSentiment analysis using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords\n",
    "\n",
    "def filter_sentence(citing_sentence):\n",
    "    if citing_sentence == None:\n",
    "        return \" \" #filtered_sentences_noNone.append(\" \")\n",
    "    \n",
    "    if citing_sentence != None:\n",
    "        citing_sentence = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", citing_sentence) #to remove citations\n",
    "        #citing_sentence = re.sub(\"[*?]\", \"\", citing_sentence) #to remove citations\n",
    "        citing_sentence = re.sub('[0-9]+', '', citing_sentence)\n",
    "        delete = [\"Introduction\", \"Background\", \"Conclusions\",\"the\", \"and\", \"therefore\", \"thus\"]#, \"\\n\", \"<\\sub>\", \"bibr\", \"ref\", \"rid\", \"type\", \"xref\"] #, \"/p\\np\\n\", \"\\p\"]\n",
    "        for word in delete:\n",
    "            citing_sentence = re.sub(word, \"\", citing_sentence) \n",
    "        #citing_sentence = re.sub(\"\\?\", \"\", citing_sentence) #to remove citations\n",
    "        citing_sentence = ' '.join([word for word in citing_sentence.split() if word not in (stopwords.words('english'))])\n",
    "        return citing_sentence # O ?????????filtered_sentences_noNone.append(sentence[0])\n",
    "\n",
    "def fit_transform_or_empty(vectorizer, data):\n",
    "    return count_vectorizer.fit_transform(data) if len(data) > 0 else csr_matrix([])\n",
    "\n",
    "#file_df = '/project/elife/data/analysis/df_1000_1000v2_prep_.csv'\n",
    "df = pd.read_csv(citing_sections_tsv, sep='\\t', encoding='utf-8')\n",
    "\n",
    "#  df['cited_DOI', 'cited_in_conclusions','cited_in_discussion',\n",
    "#    'cited_in_introduction', 'cited_in_maintext', 'citing_DOI',\n",
    "#    'conclusions_found', 'discussion_found', 'introduction_found',\n",
    "#    'maintext_found', 'reference_id', 'sentence_citing_conclusions',\n",
    "#    'sentence_citing_discussion', 'sentence_citing_intro', 'sentence_citing_maintext']\n",
    "    \n",
    "\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=5)\n",
    "#sentence = ''.join(str(string) for string in citing_sentences_list)\n",
    "#sentence = sentence.decode('utf-8')\n",
    "#u_sentence = unicode( sentence, \"utf-8\" )\n",
    "#backToBytes = u_sentence.encode( \"utf-8\" )\n",
    "\n",
    "\n",
    "\n",
    "#sentence = re.sub(r',([0-9])', '\\\\1', sentence)\n",
    "# sort out HMTL formatting of &\n",
    "#sentence = re.sub(r'&amp', 'and', sentence)\n",
    "\n",
    "citing_sentences_intro_list = df.sentence_citing_intro.dropna()\n",
    "citing_sentences_maintext_list = df.sentence_citing_maintext.dropna()\n",
    "citing_sentences_discussion_list = df.sentence_citing_discussion.dropna()\n",
    "citing_sentences_conclusions_list = df.sentence_citing_conclusions.dropna()\n",
    "    \n",
    "citing_sentences_original_intro = deepcopy(citing_sentences_intro_list)\n",
    "citing_sentences_original_maintext = deepcopy(citing_sentences_maintext_list)\n",
    "citing_sentences_original_discussion = deepcopy(citing_sentences_discussion_list)\n",
    "citing_sentences_original_conclusions = deepcopy(citing_sentences_conclusions_list) # This is a list of lists, so you need deepcopy\n",
    "\n",
    "filtered_sentences_intro_list = citing_sentences_intro_list.apply(filter_sentence)\n",
    "filtered_sentences_maintext_list = citing_sentences_maintext_list.apply(filter_sentence)\n",
    "filtered_sentences_discussion_list = citing_sentences_discussion_list.apply(filter_sentence)\n",
    "filtered_sentences_conclusions_list = citing_sentences_conclusions_list.apply(filter_sentence)        \n",
    "\n",
    "#filtered_sentences_intro_list = [filter_sentence(sentence) for sentence in citing_sentences_intro_list]\n",
    "#filtered_sentences_maintext_list = [filter_sentence(sentence) for sentence in citing_sentences_maintext_list]\n",
    "#filtered_sentences_discussion_list = [filter_sentence(sentence) for sentence in citing_sentences_discussion_list]\n",
    "#filtered_sentences_conclusions_list = [filter_sentence(sentence) for sentence in citing_sentences_conclusions_list]\n",
    "\n",
    "print(filtered_sentences_intro_list)\n",
    "#citing_sentences_intro_list.apply(filter_sentence)\n",
    "\n",
    "count_vectors_intro = fit_transform_or_empty(count_vectorizer, filtered_sentences_intro_list)\n",
    "#print(count_vectorizer.get_feature_names())\n",
    "#word_frequency_intro = count_vectors_intro.toarray()\n",
    "\n",
    "count_vectors_maintext = fit_transform_or_empty(count_vectorizer, filtered_sentences_maintext_list)\n",
    "#print(count_vectorizer.get_feature_names())\n",
    "#word_frequency_maintext = count_vectors_maintext.toarray()\n",
    "\n",
    "count_vectors_discussion = fit_transform_or_empty(count_vectorizer, filtered_sentences_discussion_list)\n",
    "#print(count_vectorizer.get_feature_names())\n",
    "#word_frequency_discussion = count_vectors_discussion.toarray()\n",
    "\n",
    "count_vectors_conclusions = fit_transform_or_empty(count_vectorizer, filtered_sentences_conclusions_list)\n",
    "#print(count_vectorizer.get_feature_names())\n",
    "#word_frequency_conclusions = count_vectors_conclusions.toarray()\n",
    "\n",
    "\n",
    "#print(word_frequency_intro)\n",
    "#print(word_frequency_maintext)\n",
    "#print(word_frequency_discussion)\n",
    "#print(word_frequency_conclusions)\n",
    "\n",
    "number_words_in_sentence_intro = np.sum(count_vectors_intro.toarray(),axis=1).tolist()\n",
    "number_words_in_sentence_maintext = np.sum(count_vectors_maintext.toarray(),axis=1).tolist()\n",
    "number_words_in_sentence_discussion = np.sum(count_vectors_discussion.toarray(),axis=1).tolist()\n",
    "number_words_in_sentence_conclusions = np.sum(count_vectors_conclusions.toarray(),axis=1).tolist()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Frequent words included (intro): \", number_words_in_sentence_intro)\n",
    "print(\"Frequent words included (maintext): \", number_words_in_sentence_maintext)\n",
    "print(\"Frequent words included (discussion): \", number_words_in_sentence_discussion)\n",
    "print(\"Frequent words included (conclusions): \", number_words_in_sentence_conclusions)\n",
    "\n",
    "print(\"maximum words in the same sentence in introduction =\", max(number_words_in_sentence_intro))\n",
    "print(\"maximum words in the same sentence in maintext =\", max(number_words_in_sentence_maintext))\n",
    "print(\"maximum words in the same sentence in discussion =\", max(number_words_in_sentence_discussion))\n",
    "print(\"maximum words in the same sentence in conclusions =\", max(number_words_in_sentence_conclusions))\n",
    "\n",
    "\"\"\"\n",
    "df_sentences_intro = pd.DataFrame({\"citing_intro_sentence\" : filtered_sentences_intro_list, \n",
    "                                 \"number_frequent_words\" : number_words_in_sentence_intro})\n",
    "\"\"\"\n",
    "# length of sentence\n",
    "\n",
    "sentence_length_intro = []\n",
    "for sentence in citing_sentences_intro_list:\n",
    "    if sentence == None:\n",
    "        sentence_length_intro.append(0)\n",
    "    if sentence != None:\n",
    "        sentence_length_intro.append(len(sentence))\n",
    "\n",
    "df_sentences_intro = pd.DataFrame({\"frequent_words\" : number_words_in_sentence_intro,\n",
    "                                \"citing_sentence\" : citing_sentences_original_intro,\n",
    "                                \"sentence_length\": sentence_length_intro})\n",
    "\n",
    "\n",
    "sentences_toCheck = df_sentences_intro[(df_sentences_intro[\"frequent_words\"] == max(df_sentences_intro[\"frequent_words\"]))] #['sentence_citing_intro']\n",
    "min_length= min(sentences_toCheck[\"sentence_length\"][:])\n",
    "print(\"min_length: \", min_length)\n",
    "\n",
    "sentence = sentences_toCheck[sentences_toCheck['sentence_length'] == min_length][\"citing_sentence\"]\n",
    "\n",
    "# To print the full content\n",
    "sentence.to_csv(sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(citing_sections_tsv, sep='\\t', encoding='utf-8')\n",
    "\n",
    "citing_sentences_intro= df['sentence_citing_maintext']\n",
    "citing_sentences_intro_list = citing_sentences_intro.tolist()\n",
    "print(citing_sentences_intro_list[1])\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=5)\n",
    "#sentence = ''.join(str(string) for string in citing_sentences_list)\n",
    "#sentence = sentence.decode('utf-8')\n",
    "#u_sentence = unicode( sentence, \"utf-8\" )\n",
    "#backToBytes = u_sentence.encode( \"utf-8\" )\n",
    "\n",
    "\n",
    "\n",
    "#sentence = re.sub(r',([0-9])', '\\\\1', sentence)\n",
    "# sort out HMTL formatting of &\n",
    "#sentence = re.sub(r'&amp', 'and', sentence)\n",
    "\n",
    "# If you want to avoid to go through all the papers, you can select those with introductions by\n",
    "# by replacing citing_sentences_intro_list by df_intro_filtered.tolist()\n",
    "\n",
    "\n",
    "citing_sentences_original = deepcopy(citing_sentences_intro_list) # This is a list of lists, so you need deepcopy\n",
    "#print(citing_sentences_original[19])\n",
    "filtered_sentences_noNone = []\n",
    "filtered_sentences = citing_sentences_intro_list[:]\n",
    "print(filtered_sentences[0:1])\n",
    "for sentence in filtered_sentences:\n",
    "    if (type(sentence) == float):\n",
    "        filtered_sentences_noNone.append(\" \")\n",
    "    if (type(sentence) != float):\n",
    "        #print(\"######################### \", type(sentence), sentence)\n",
    "        delete = [\"Introduction\", \"Background\", \"the\", \"and\"]\n",
    "        for word in delete:\n",
    "            sentence = re.sub(word, \"\", sentence) \n",
    "        sentence = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", sentence) #to remove cititations\n",
    "        sentence = ' '.join([word for word in sentence.split() if word not in (stopwords.words('english'))])\n",
    "        filtered_sentences_noNone.append(sentence)\n",
    "\n",
    "count_vectorizer.fit_transform(filtered_sentences_noNone) #backToBytes\n",
    "#print(citing_sentences_original[19]) \n",
    "    \n",
    "print(count_vectorizer.get_feature_names())\n",
    "count_vectors = count_vectorizer.transform(filtered_sentences_noNone)\n",
    "\n",
    "word_frequency = count_vectors.toarray()\n",
    "print(word_frequency)\n",
    "number_words_in_sentence = np.sum(count_vectors.toarray(),axis=1).tolist()\n",
    "\n",
    "print(\"Frequent words included: \", number_words_in_sentence)\n",
    "\n",
    "\n",
    "#print(citing_sentences_intro_list)\n",
    "print(\"maximum words in the same sentence =\", max(number_words_in_sentence))\n",
    "\n",
    "#df_introCiting = pd.DataFrame({\"citing_intro_sentence\" : filtered_sentences_noNone, \n",
    "#                                \"number_frequent_words\" : number_words_in_sentence})\n",
    "#print(df_introCiting)\n",
    "\n",
    "# length of sentence\n",
    "sentence_length = []\n",
    "for sentence in citing_sentences_intro_list:\n",
    "    if (sentence == None) | (sentence == np.nan):\n",
    "        sentence_length.append(0)\n",
    "    if (sentence != None) & (sentence != np.nan):\n",
    "        sentence_length.append(len(str(sentence)))\n",
    "\n",
    "\n",
    "\n",
    "df_introCiting = pd.DataFrame({\"frequent_words\" : number_words_in_sentence,\n",
    "                                \"citing_sentence\" : citing_sentences_original,\n",
    "                                \"sentence_length\": sentence_length})\n",
    "\n",
    "\n",
    "sentences_toCheck = df_introCiting[(df_introCiting[\"frequent_words\"] == max(df_introCiting[\"frequent_words\"]))] #['sentence_citing_intro']\n",
    "min_length= min(sentences_toCheck[\"sentence_length\"][:])\n",
    "\n",
    "sentence = sentences_toCheck[sentences_toCheck['sentence_length'] == min_length][\"citing_sentence\"]\n",
    "\n",
    "# To print the full content\n",
    "sentence.to_csv(sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "df_sentiment = df_introCiting.copy()\n",
    "#ss = sid.polarity_scores(df_sentiment['citing_sentence'][4][0]) #if None => error\n",
    "#print(ss)\n",
    "\n",
    "df_sentiment2 = filtered_sentences_noNone\n",
    "#ss2 = sid.polarity_scores(df_sentiment2[2])\n",
    "ss2 = sid.polarity_scores(df_sentiment2[0])\n",
    "\n",
    "sentiment_scores_list = []\n",
    "for sentence in df_sentiment2:\n",
    "    ss2 = sid.polarity_scores(sentence) # from twython  package\n",
    "    sentiment_scores_list.append(ss2)\n",
    "df_sentiment_scores = pd.DataFrame(sentiment_scores_list)\n",
    "\n",
    "    #df_sentiment_scores.append(ss2)\n",
    "#df_sentiment2\n",
    "\n",
    "\n",
    "df_sentiment = df_sentiment.join(df_sentiment_scores)\n",
    "\n",
    "df_sentiment\n",
    "\n",
    "# for i, tweet in df_sentiment['text'].iteritems():\n",
    "#     ss = sid.polarity_scores(str(tweet))\n",
    "#     for k in sorted(ss):\n",
    "#         df_sentiment.loc[i, k] = ss[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(len(df_sentiment))\n",
    "df_sentiment['compound']\n",
    "\n",
    "#plt.close()\n",
    "#comp = plt.scatter(x,df_sentiment['compound'])\n",
    "pos = plt.scatter(x,df_sentiment['pos'], color = 'g')\n",
    "neu = plt.scatter(x,df_sentiment['neu'], color = 'y')\n",
    "neg = plt.scatter(x,df_sentiment['neg'], color = 'r')\n",
    "\n",
    "plt.legend((pos, neu, neg),('positive', 'neutral','negative'), loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"List of paper\", fontsize=14)\n",
    "plt.ylabel(\"Sentiment score\", fontsize=14)\n",
    "\n",
    "plt.savefig(\"sentiment.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
